\section{Analyses}

\subsection{Statistical Models}

\subsubsection{Study 1}

\paragraph{Bayesian Approach}

In answering Study 1's research question, we propose a Bayesian procedure for estimating the missing 2020 exam data. Since educational assessment maps students' learning outcomes ($L$) onto a numerical scale ($M$), a valid and reliable assessment inventory should show high degrees of agreement between $L$ and $M$, that is, $\p{M \mid L}$ (“given the learning, how likely these grades appear”) shall be close to $1$. Stakeholders, on the other hand, are more interested in knowing “given the grades, how likely there is learning” (i.e., $\p{L \mid M}$). These two interests can be linked via the Bayes formula:
\begin{equation}
    \p{L \mid M} = \frac{\p{M \mid L} \p{L}}{\text{normalising constant}},
\end{equation}
where $\p{L}$ is a ``prior belief'' of learning that is to be updated by the exam results. In this study, teacher-assigned grades can serve as the prior. Properties of $\p{M \mid L}$ can be ascertained from earlier years' exam papers as exam questions are reused. The normalizing constant can calculated using the law of total probability.

\subsubsection{Study 2}

\paragraph{Descriptive Statistics}

In a first step, we plan to gain insight into what conditions did Norwegian students study at home during school closures and to get an overview of characteristics of the schools and municipalities, we will report the means and standard deviations of the independent and dependent variables as well as the moderators on student-, school- and municipality-levels. In the second step, we will use ANOVA to test whether the schools differ significantly in key variables (e.g., SES) over the last five years (i.e., 2015 to 2019) before COVID-19 school closures. We consider schools differ with respect to a key variable if differences were evident in three or more years between 2015 and 2019. In a third step, we will describe students exempted and not exempted from the national tests separately (e.g., by age, sex, and SES).

\paragraph{Difference-in-Difference Approach}

We will analyse students' learning progression between 2019 (students' reading and mathematics achievement in national tests in Year 8) and 2020 (same measure in Year 9) using a difference-in-difference (DiD) approach \parencite{angrist:2009} using \textsf{R} \parencite[e.g., ][]{brumback:2021}, similar to the approach in \textcite{engzell:2021}. We will subsequently compare students' learning progression between 2019 and 2020 (Period 1) with that in the preceding five periods. Comparisons were limited from 2015 onwards since systems incompatible with the current \href{https://www.ssb.no/en/utdanning/grunnskoler/statistikk/nasjonale-prover}{score points} were used before 2014.

First, we will calculate the differences between students' Year 9 and 8 national test grades using the following equation:
    \[ \Delta y_i^{(k+1)-k} = y_i^{k+1} - y_i^k, \]
where $y_i^k$  is the Year 8 national test grade of Student $i$ in calendar year $k$, whereas $k+1$ refers to the subsequent year when this student entered Year 9. We will calculate the difference scores $\Delta y_i^{2015-2014}$,  $\Delta y_i^{2016-2015}$, $\Delta y_i^{2017-2016}$, $\Delta y_i^{2018-2017}$, $\Delta y_i^{2019-2018}$ (five difference scores before COVID-19 school closures) and $\Delta y_i^{2020-2019}$ (difference scores in Period 1).

Next, we will compare these difference scores using two-level linear mixed effect models with cross-level interactions to account for students nested within schools. We will test H1 using a basic model shown in \crefrange{eqn:21}{eqn:23} that contain only one Level 2 variables \vn{dur} and no Level 1 covariates:

\noindent\textbf{Level 1}
\begin{equation}\label{eqn:21}\tag{2.1}
    \Delta y_{ij} = \beta_{0j} + \epsilon_{ij},
\end{equation}
where index $ij$ represents Student $i$ within School $j$, whose error term $\epsilon_{ij} \sim \mathcal{N}(0, \sigma_\epsilon^2)$. In this example
$\Delta y_{ij} = \vn{np\_math9}_{ij} - \vn{np\_math8}_{ij}$.

\noindent\textbf{Level 2}
\begin{equation}\label{eqn:22}\tag{2.2}
    \beta_{0j} = \gamma_{00} + \gamma_{01} \vn{dur}_j + u_{0j}.
\end{equation}
Notice that \vn{dur} is indexed by $j$ since it varies at the school-level. $u_{0j} \sim \mathcal{N}(0, \sigma_{u_0}^2)$. For the periods before COVID-19 school closures, $\vn{dur} = 0$.

\noindent\textbf{Total composite formula}
\begin{equation}\label{eqn:23}\tag{2.3}
    \Delta y_{ij} = \gamma_{00} + \gamma_{01} \vn{dur}_j+ u_{0j} + \epsilon_{ij}
\end{equation}
The slope $\gamma_{01}$ is of interest to RQ2.

\crefrange{eqn:31}{eqn:33} expand our models by including Level 1 and 2 variables:

\noindent\textbf{Level 1}
\begin{equation}\label{eqn:31}\tag{3.1}
    \Delta y_{ij} = \beta_{0j} + \beta_{1j} \vn{sex}_{ij} + \beta_{2j} \vn{age}_{ij} + \beta_{3j} \vn{atipcu}_{ij} + \beta_{4j} \vn{stp\_math}_{ij} + \epsilon_{ij},
\end{equation}
with notations similar to those in \cref{eqn:21}.

\noindent\textbf{Level 2}
\begin{equation}\label{eqn:32}\tag{3.2}
    \begin{aligned}
    \beta_{0j} &= \gamma_{00} + \gamma_{01} \vn{dur}_j + u_{0j}\\
    \beta_{3j} &= \gamma_{30} + \gamma_{31} \vn{dur}_j + u_{3j}\\
    \beta_{4j} &= \gamma_{40} + \gamma_{41} \vn{dur}_j + u_{4j}.
    \end{aligned}
\end{equation}
with $u_{3j} \sim \mathcal{N}(0, \sigma_{u_3}^2)$, and $u_{4j} \sim \mathcal{N}(0, \sigma_{u_4}^2)$ in addition to notations in \cref{eqn:22}.

\noindent\textbf{Total composite formula}
\begin{equation}\label{eqn:33}\tag{3.3}
	\begin{aligned}
		\Delta y_{ij} &= \gamma_{00} + \gamma_{01} \vn{dur}_j + u_{0j} + \beta_{1j} \vn{sex}_{ij} + \beta_{2j} \vn{age}_{ij}\\
            &+ (\gamma_{30} + \gamma_{31} \vn{dur}_j + u_{3j}) \vn{atipcu}_{ij} + (\gamma_{40} + \gamma_{41} \vn{dur}_j + u_{4j}) \vn{stp\_math}_{ij} + \epsilon_{ij}\\
			&= \gamma_{00} + \gamma_{01} \vn{dur}_j + \beta_{1j} \vn{sex}_{ij} + \beta_{2j} \vn{age}_{ij} + \gamma_{30} \vn{atipcu}_{ij}\\
            &+ \textcolor{red}{\gamma_{31}} \vn{dur}_j \vn{atipcu}_{ij} + \gamma_{40} \vn{stp\_math}_{ij} + \textcolor{blue}{\gamma_{41}} \vn{dur}_j \vn{stp\_math}_{ij}\\
            &+ u_{3j} \vn{atipcu}_{ij} + u_{4j} \vn{stp\_math}_{ij} + u_{0j} + \epsilon_{ij}
	\end{aligned}
\end{equation}

The coefficients of the cross-level interaction terms between the duration of school closures and students' SES ($\textcolor{red}{\gamma_{31}}$) and students' previous achievement ($\textcolor{blue}{\gamma_{41}}$) represent the moderation effects asked by \textcolor{red}{RQ3} and \textcolor{blue}{RQ4} respectively.

Following the recommendations in \textcite{enders:2007}, we will centre our variables to enhance interpretation. Separate models will be run for reading and mathematics.

The key idea for the DiD approach is that, in the absence of treatments (here, COVID-induced school closures), the trends of the dependent variable (the national test results) should be comparable between the treated (after COVID) and the untreated group (before COVID) \parencite{angrist:2009}. Since birth years from a decade ago shall not anticipate the arrival of COVID-19, the underlying student characteristics such as IQs and learning capabilities should remain stable across the cohorts. That is, in the absence of COVID-19, we shall expect no significant differences between the amount of learning growth of the 2020 students and that of the 2019 students. Should such comparability be broken, the ``treatment'' (school closure) must carry the explanatory power for such differences. As the validity of the DiD approach relies on the assumption of equal trends, we will use the five periods before COVID-19 to compare $\Delta y_i^{(k+1)-k}$. If the $\Delta y_i^{(k+1)-k}$ before COVID are similar, it is reasonable to expect such similarity to extend into $\Delta y_i^{2020-2019}$ in the absence of the pandemic.  Similarly, we compare whether the trends are comparable with respect to students' SES (i.e., \vn{atipcu}) and earlier achievement (e.g., \vn{stp\_math}) before COVID-19.

To gain more insight into students' family situation and household, as well as the circumstances of learning at home, we will explore additional individual-level and school-level variables (see \cref{tab:var}) using \crefrange{eqn:21}{eqn:23} as a starting point (see the Exploratory Analyses section).

We plan on using \textsf{R} \parencite{r} for data management and simple regressions and \textsf{Mplus} \parencite{mplus} for multi-level analyses.

\paragraph{Nesting of Data}

Study 2 presents data that are hierarchical in nature. Different nesting structures can be considered for our analyses (e.g., students are nested in classes, in schools, or in families), depending on the research questions (``What is of interest?'') and on the level at which key independent variables reside \parencite{scott:2013}. In our study, school closure duration is the key independent variable, therefore suggesting two possible nesting structures:
\begin{seriate}
    \item students nested in municipalities, or
    \item students nested in schools.
\end{seriate}%
Option (a) makes sense as municipalities in Norway are responsible for making policies related to lower secondary schools including COVID directories. As scientific knowledge about COVID accumulates, large municipalities such as the capital Oslo initiated differentiated school closure requirements based on a coloured alert system. By the later stage of the pandemic, it makes better sense to use schools as the Level 2 unit since variation started to emerge within the same municipality. We therefore decide to adopt students nesting in schools for our analyses. We will assess the significance of the nesting structure using intraclass correlations \parencite[$\text{ICC}_1$, ][]{ludtke:2009}.

\subsection{Effect Size}

In Study 2, learning growth is operationalised as the difference in national test results between Year 8 and Year 9. We then express effect sizes in percentiles using the SD-based metrics Cohen's $d$ \parencite{cohen:1988}, with all effect sizes being considered informative (see explanations in hypotheses). We expect negative effects of school closures on student achievement. Prior studies signalled a learning loss of approximately $d=-0.10\ SD$ in Period 1 for reading and mathematics (Hammerstein et al., 2021). We further expect the effect sizes to be small since Norway's national tests measure youth's basic competencies such as numeracy and literacy---skills that should have stabilised by Year 9 and not immediately sensitive to school closures. We will use earlier findings as benchmarks when judging the importance of effect sizes. \textcite{betthauser:2022} found a learning loss of $d=-0.17$, 95\% CI $[-0.22,\ -0.13]$ early in the pandemic, equivalent to 42 percent of average learning during a school year in the absence school closures \parencite[teachers typically can attain between $d=0.20$ and $d=0.40$ per year; ][]{hattie:2009}. \textcite{hammerstein:2021} found a learning loss of $d=-0.005\ SD$ to $-0.05\ SD$ per week, which can be interpreted as an average summer learning loss. Based on these findings and the fact that Norway's national tests are general competency-based in contrast to curriculum-based (e.g., in the Netherlands), we expect a small effect size of approximately $d=-0.2/52=-0.0038\ SD$ per week or smaller for Period 1.

\subsection{Statistical Power}

Since Norway's national register data preserve the entire population with minimal missing values, we expect our study to have sufficiently high statistical power with significance output for all inferential parameters. The challenge following the analyses is to interpret which magnitudes of effect sizes are of practical relevance for policy considerations (see the Effect Size section).

\subsection{Inference Criteria}

Conventional statistical tests operate on sampling distributions and are not directly applicable to our studies involving the entire population. We are nevertheless able to employ the conventional Type I error criterion of $\alpha = .05$ and the Benjamini-Hochberg correction \parencite{benjamini:1995} procedure to adjust for multiple comparisons.

\subsection{Assumption Violation/Model Non-convergence}

Variables with severely non-normal distributions such as income will be subject to log transformations in order to enhance normality. Since no latent construct is involved in current studies and our data size is sufficiently large, we do not expect model identification problems or non-convergence risks.

\subsection{Reliability and Robustness Testing}

We will take several measures to verify the robustness of our results.

\subsubsection{Study 1}

We propose two different methods for handling missing values. The first and primary approach is MI (see the Missing Data section). We then wish to corroborate MI with the full information maximum likelihood (FIML) approach \parencite{graham:2012,vanbuuren:2018} thanks to its ability to produce unbiased estimates once all variables associated with missingness are included in the estimation \parencite{graham:2012,schafer:2002}. In executing FIML for independent and dependent variables, we will include demographic and SES variables in addition to those that are already included in the substantive model \parencite[saturated correlates models, ][]{graham:2003}.

\subsubsection{Study 2}

We would like to first of all implement robustness checks at the variable level. In order to gain deeper insight into different facets of SES \parencite{apa:2017,avvisati:2020,oconnell:2019}, students' social capital can be operationalised using alternative measures such as parents' education or using an index approach similar to that in PISA studies \parencite{oecd:2019}. Similarly, students' previous achievement in Norwegian (\vn{stp\_norw}) and mathematics (\vn{stp\_math}) can be corroborated using their attainment record in social (\vn{stp\_socs}) and natural sciences (\vn{stp\_nats}), respectively. In addition, missing data treatment described in Study 1 can also be applied to Study 2.

Next, our robust checks focus on the modelling level. Our central approach (see the Analyses section) is highly influenced by the \textcite{engzell:2021} paper. An alternative regression design can be found in \textcite{angrist:2009} for validating effect estimations \parencite[see also ][]{brumback:2021}. \poscite{angrist:2009} DiD regression models may include time-varying covariates and are implemented using long data format (see, in particular, pp. 236—239). Furthermore, hierarchical data structure can be re-constructed as students nested in 428 Norwegian municipalities as an alternative to schools. Should the data suggest variations at another level that we are not currently aware of, $\text{ICC}_1 \geq .05$ \parencite{lebreton:2008}, we will include this level in our analyses as another robustness check.

If further needs for robustness arise in analyses or during the review process, we will carry out these checks should the benefits sufficiently justify the computational costs.

\subsection{Exploratory Analysis}

Further exploratory analyses can be conducted should they be theoretically plausible. First, we plan on analysing Period 2 after the register data update. Similar to Period 1, we will calculate the difference in Year 8 and Year 9 student achievement in the national tests $\Delta y_i^{2021-2020}$ and explore how Period 2 relates to Period 1 and the other periods before COVID school closures using DiD. Given the cumulative nature of learning (\citenp{hammerstein:2021}; see also \citenp{shuell:1986}), one can reasonably expect potential learning losses to grow in the long run---a trend that has already been reported by \textcite{betthauser:2022} that learning deficit early in the pandemic persisted over time. On the other hand, government initiatives such as training programs and provision of free digital devices may cushion the adverse effect from sudden disruptions. As all education participants adapt to online learning, Period 2 may well exhibit different patterns than Period 1.

Second, we may compare learning progressions across subjects. Previous studies on the effects of school closures on student achievement reported few differences between reading and mathematics \parencite{hammerstein:2021}. It is unclear, however, whether similar patterns apply to Norway due to its unique education and assessment systems, such as repeating identical tests one year apart and focusing on general competencies rather than on curricula. One's general literacy and numeracy levels are reasonably expected to have stabilised by Year 8---if differential effects were to happen, we expect them to be larger in mathematics than in reading because numeracy depends more heavily on purposeful training than language skills.

Third, we may explore further variables. We aim to relate COVID school closures to learners' SES, their family structure as well as housing conditions that may have served as strong levers during home learning. For instance, we may explore additional individual-level (e.g., immigrant status, COVID-19 infections, number of siblings), school-level (e.g., school type, proportion of immigrants), or household-related variables (e.g., floor space per person in the household, working hours of parents; nr. 15 in \cref{tab:var}).

We will respond to expert advice by including additional variables that are available in our register database in order to better reflect theories and/or strengthen model robustness.
