\section{Results}

\subsection{Descriptive Statistics}

\cref{tab:descriptive} summarised key information about the 18 \textsc{gpa} subjects examined by this study, including the number of valid entries, grade distributions, and links to official documentation. It is firstly noticeable that data missing rates differed significantly across modes of assessment. Teacher-assigned grades carried small missing percentages most under 5 percent, hence imposing little concerns over estimation bias. Although written- and oral-exams had large missing percentages, this was the effect of the equal-probability sampling procedures. Under planned missingness, the observed grades represent unbiased estimates of true grades despite only 1/3 or 1/5 of the students were studied.

Secondly, grade distributions differed both between- and within-modes of assessment. A large number of grade counts clustered around Grade 3 and 4 for external exams, whereas teacher-assigned grades peaked at different bands depending on the subject, with \textsc{math} mainly covering Grade 2 to 4 while \textsc{food} covering largely Grade 4 and 5.

\subsection{Subject Difficulties}

\subsubsection{Overall Difficulties}
\textsc{gpa} subjects' overall difficulties are shown in \cref{fig:expected}. Using \textsc{math} as an example, the horizontal axis of Panel A represents students' latent competencies, ranging from low ($\theta=-10$) to high ($\theta=5$), and the vertical axis represents grades randing from $1$ to $6$. Students with low competencies are expected to receive Grade 1 while Grade 6 is reserved to students with very high competencies. Mapping every competency level to its expected grade yields the sigmoid curve in Panel A. Furthermore, there exists a median student, who evenly divides \textsc{math}'s observations into 50\% below, and 50\% above him/her, whose $\theta$ is defined as zero. Tracing this median student's expected score from the curve in Panel A, one reads a grade of 3.64 as the \emph{overall difficulty} for \textsc{math}. Repeating this procedure for all 18 GPA subjects gave rise to the scatter plot in Panel B. Subject with low expected grades such as \textsc{math} are more difficult while \textsc{phed} and \textsc{food} are easy subjects evidenced by the high expected grades from median students.

Ranked by overall difficulties, teacher-assigned grades appeared to align themselves along the \textit{manu}--\textit{mente} dichotomy. A median student is expected to receive a score one grade lower in the most difficult subject \textsc{math} than from the easiest one \textsc{phed}. Written exams are more difficult than oral exams, with \textsc{nor\_w} being more difficult than teacher-assigned \textsc{math}. Oral English exam, in contrast, is comparable in difficulty to \textit{mente} subjects such as teacher-assigned \textsc{food}.

\subsubsection{Grade-level Difficulties}
This study operationalises grade-level difficulties using difficulty thresholds. For a polytomous \textsc{irt} item such as \textsc{math}, a category characteristic curve (\textsc{ccc}) describes the likelihood a particular grade is received by students with varying competency levels. The $P1$ curve in Panel A \cref{fig:grade_level}, for example, states that Grade 1 is awarded to students with low competencies almost surely (probability approaching 1) but to those with high competencies almost never (probability approaching 0). Similarly, the $P2$ curve suggests that Grade 2 is most likely to be awarded to students with competencies between approximately $\theta=[-6, -1]$ but low probabilities outside this domain. The intersection between $P1$ and $P2$ marks a difficulty threshold $\delta_1$, above which the next grade is more likely. Six \textsc{ccc}s produce five difficulty thresholds $\delta_1, \dots, \delta_5$, which concisely summarise each subject's \emph{grade-level difficulties}. Repeating this procedure to all 18 \textsc{gpa} subjects produces Panel B.

Among teacher-assigned grades, the competency demands for receiving a particular grade differed widely depending on the low- and high-end of the grading scale. High consistency was observed at the $\delta_5$-level where all subjects required students to have high competencies ($\theta \approx 2.5$) to transitions from Grade 5 to the top grade 6. As one moves down the grade ladder, however, the difficulty gap expanded to more than one grade between the most difficult subject and the easiest one such that a Grade 3 in \textsc{math} is more comparable to a Grade 4 in \textsc{food}. Lastly, the lengthening 95\% confidence intervals in $\delta_1$ suggests that teachers did not fully utilise the entire grade scale, especially for the \textit{manu} subjects---an observation corroborated by the grade distributions in \cref{tab:descriptive}.

\subsection{Model Fit Measures and Information Curves}

\cref{fig:fit} visualises the Rasch model fit statistics using the 2019 Year 10 \textsc{gpa} data. A model with perfect fit would generate an information weighted fit (infit) and unweighted fit mean square (outfit) of $1$ \parencite{wu:2016}. Infit and outfit mean squares below $1$ suggest overfit where the item is more discriminating than the average item discrimination. Resultantly, \textcite{wu:2016} consider high quality items (\textit{mente} subjects) to have mean squares less than $1$ even though some of these items may be deemed as misfitting the model. \textsc{gpa} subjects with mean squares much greater than $1$ are deemed poorer \textsc{irt} items. Under these criteria, teacher-assigned grades for \textit{manu} subjects \textsc{hand}, and \textsc{phed} showed poor model fit, as well as oral English exam grades.

Lastly, \cref{fig:info} presents the information curves (left scale, blue) for the 18 \textsc{gpa} subjects. An information curve plots the information function against the latent competency. The information function is the expected information gained from a student's response to an item given their competency level. \cref{fig:info} also displays the standard error curves (right scale, red) that communicate the precision of each Rasch item over the competency range. The information and standard error curves jointly suggest that the Rasch model used in this study provided strong explanatory power and high precision over the mid-range of the latent competency scale where most students reside.
