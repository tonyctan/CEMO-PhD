Conventional item response theory (IRT) relies on the "ignorability" assumption behind any missing data process. Such foundation appears no longer solid in Norwegian high school graduation tests where observed grades are the results of students self-selecting into subjects with the highest expected payoffs--yielding under-estimations of subject difficulties and over-estimations of person competencies. Consensus remains scarce over the appropriate IRT procedures in the presence of missing-not-at-random (MNAR) data. This thesis compares joint modelling (JM) against multiple imputation (MI) approaches in producing unbiased IRT parameters with non-ignorable missing data. Using 2019 Norwegian senior high school test results as the main data source, I show that MI is superior to JM in terms of bias reduction and efficiency. A two-stage procedure for estimating subject difficulties and graduates' competencies are proposed for the purpose of enhancing fairness in educational assessment.

Since conventional item response theory (IRT) procedure assumes ignorablelity among the observed data, students' self-selection into subjects with the highest expected payoffs causes systematic biases in estimation. Using 2019 Norwegian senior high school test results as my main data source, I estimate the directions and magnitudes of IRT estimation biases for both item- and person-parameters using the joint modelling approach. This thesis provides empirical evidence for assessment fairness during university admissions.

Using 2019 Norwegian senior high school grades, my master thesis applies item response theory to estimate GPA subject difficulties, with explicit consideration of self-selection bias.