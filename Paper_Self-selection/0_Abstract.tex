\section{Abstract}

Conventional item response theory (IRT) relies on the "ignorability" assumption behind any missing data. Such foundation appears no longer solid in Norwegian high school graduation tests where observed grades result from students choosing subjects with highest payoffs. Such self-selection yields under-estimations of subject difficulties and over-estimations of person competencies. Consensus remains scarce over the appropriate IRT procedures in the presence of missing-not-at-random (MNAR) data. This paper compares joint modelling (JM) against multiple imputation (MI) approaches in producing unbiased IRT parameters with non-ignorable missing data. Using 2019 Norwegian senior high school grades, we show that MI is superior to JM in terms of bias reduction and efficiency. A two-stage procedure for estimating subject difficulties and graduates' competencies are proposed for the purpose of enhancing fairness in educational assessment.

% For Sverre's project page:
% Since the conventional item response theory (IRT) procedure assumes ignorability among missing data, students' self-selection into subjects with the highest expected payoffs causes systematic biases in estimation. Using 2019 Norwegian senior high school grades, I estimate the directions and magnitudes of IRT estimation biases for both item- and person-parameters using the joint modelling approach. This thesis provides empirical evidence for assessment fairness during university admissions.

% Using 2019 Norwegian senior high school grades, my master thesis applies item response theory to estimate GPA subject difficulties, with explicit consideration of self-selection bias.
