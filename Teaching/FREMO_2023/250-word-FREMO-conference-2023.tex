The Programme for International Student Assessment (PISA) have produced rich data that captured participating countries' schooling experience among students, teachers, parents, and principals. In recent cycles, the transition into computer-based assessment has made available to researchers process data such as response time (RT) in addition to test taker responses \parencite{oced:2020a}. In this study, I present one application of RT data as a quality assurance device by identifying rapid item responses. Although students have given \emph{valid} answers as per the codebook, overly short response time questions whether their answers were \emph{genuine} replies to test stimuli.

Using the 2018 PISA questionnaire data, I would like to firstly identify observations with response time below certain threshold (e.g., 20 seconds for a 20-item question). I will then re-estimate the gender gaps in ``enjoyment of reading'' and ``competitiveness/cooperativeness'' indices \parencite{oced:2020b} by quarantining the rapid response subsamples. I hypothesise boys to be over-represented in the rapid response subsample, whose removal will alter the gender gap estimates.

Not all students take their PISA tests with equal diligence. This line of research is important because it provides a new perspective on data quality. By not accepting all answers at their face value but also considering their generating process, process data have the potential to mitigate the ``rubbish in, rubbish out'' problem in international large-scale assessment. I will also discuss contributions of other types of process data such as eye tracking for quality enhancement purposes as well as the surrounding ethical, legal, and practical implications.
