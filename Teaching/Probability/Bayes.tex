\documentclass[a4paper,11pt]{apa7}

\usepackage{amsmath,amssymb}

\pagestyle{plain}

\begin{document}

In 1749, the Scottish philosopher \textbf{David Hume} dealt a serious blow to a fundamental belief of Christianity by publishing an essay on the nature of cause and effect. In it, Hume argues that ``cause and effects are discoverable, not by reason, but by experience''. In other words, we can never be certain about the cause of a given effect. For example, we know from experience that if we push a glass off the side of a table, it will fall and shatter, but this does not prove that the push caused the glass to shatter. It is possible that both the push and the shattering are merely correlated events, reflecting some third, and hitherto  unknown, ultimate cause of both. Hume's argument was unsettling to Christianity because God was traditionally known as the First Cause of everything. The mere fact that the world exists was seen as evidence of a divine creator that caused it to come into existence. Hume's argument meant that we can never deal with \emph{absolute} causes; rather, we must make do with \emph{probable} causes. This is weakened the link between a divine creator and the world that we witness and, hence, undermined a core belief of Christianity.

Around this time the Reverend \textbf{Thomas Bayes} of Tunbridge Wells began to ponder whether there might be a mathematical approach to cause and effect.

Thomas Bayes was born around 1701 to a Presbyterian minister, Joshua Bayes, who oversaw a chapel in London. The Presbyterian Church at the time was a religious denomination persecuted for not conforming to the governance and doctrine of the Church of England. Being a non-conformist, the young Bayes was not permitted to study for a university degree in England and so enrolled at the University of Edinburgh, where he studies theology. After university, Bayes was ordained as a minister of the Presbyterian Church by his clergyman father and began work as an assistant in his father's ministry in London. Around 1734, Bayes moved south of London to the wealthy spa resort town of Tunbridge Wells and became minister of the Mount Sion chapel there.

Around this time, Bayes began to think about how to apply mathematics, specifically probability theory, to the study of cause and effect. Specifically, Bayes wanted a mathematical way to go from an effect back to its cause. To develop his theory, he proposed a thought experiment: he imagined attempting to guess the position of a ball on a table. Not perhaps the most enthralling of though experiment, but sometimes clear thinking is boring. Bayes imagined that he had his back turned to the table, and asks a friend to throw a cue ball onto its surface. He then asks his friend to throw a second ball, and report to Bayes whether it landed to the left or right of the first. If the ball landed to the right of the first, then Bayes reasoned that the cue ball is more likely to be on the left-hand side of the table, and vice versa if it landed to its left. Bayes and his friend continue this process where, each time, his friend throws subsequent balls and reports which side of the cue ball his throw lands. Bayes' brilliant idea was that, by assuming all positions on the table were equally likely a priori, and using the results of the subsequent throws, he could narrow down the likely position of the cue ball on the table. For example if all throws landed to the left of the cue ball, it was likely that the cue ball would be on the far right of the table. And, as more data (the result of the throws) was collected, he became more and more confident of the cue ball's position. He had gone from an effect (the result of the throws) back to a probable cause (the cue ball's position)!

Bayes's idea was discussed by members of the Royal Society, but it seems that Bayes himself perhaps was not so keen on it, and never published this work. When Bayes died in 1761 his discovery was still languishing between unimportant memoranda, where he had filed it. It took the arrival of another, much more famous, clergyman to popularise his discovery.

\textbf{Richard Price} was a Welsh minister of the Presbyterian Church, but was also a famous political pamphleteer, active in liberal causes of the time such as the American Revolution. He had considerable fans in America and communicated regularly with Benjamin Franklin, John Adams and Thomas Jefferson. Indeed, his fame and adoration in the United States reached such levels that in 1781, when Yale University conveyed two degrees, it gave one to George Washington and the other to Price. Yet today, Price is primarily known for the help that he gave his friend Bayes.

When Bayes died, his family asked his young friend Richard Price to examine his mathematical papers. When Price read Bayes's work on cause and effect he saw it as a way to counter Hume's attack on causation (using an argument not dissimilar to the Intelligent Design hypothesis of today), and realised it was worth publishing. He spent two years working on the manuscript--correcting some mistakes and adding references--and eventually sent it to the Royal Society with a cover letter of religious bent. Bayes for his (posthumous) part of the paper did not mention religion. The Royal Society eventually published the manuscript with the secular title, \textit{An Essay towards solving a Problem in the Doctrine of Chances}. Sharon McGrayne--a historian of Bayes--argues that, by modern standards, Bayes' rule should be known as the Bayes-Price rule, since Price discovered Bayes' work, corrected it, realised its importance and published it.

Given Bayes' current notoriety, it is worth noting what he did not accomplish in his work. He did not actually develop the modern version of Bayes' rule that we use today. He just used Newton's notation for geometry to add and remove areas of the table. Unlike Price, he did not use the rule as proof for God, and was clearly not convinced by his own work since he failed to publish his papers. Indeed, it took the work of another, more notable, mathematician to improve on Bayes' first step, and to elevate the status of inverse probability (as it was know at the time).

\textbf{Pierre Simon Laplace} was born in 1749 in Normandy, France, into a house of respected dignitaries. His father, Pierre, owned and farmed the estates of Maarquis, and was Syndic (an officer of the local government) of the town of Beaumont. The young Laplace (like Bayes) studied theology for his degree at the University of Caen. There, his mathematical brilliance was quickly recognised by others, and Laplace realised that maths was his true calling, not the priesthood. Throughout his life, Laplace did important work in many fields including analysis, differential equations, planetary orbits and potential theory. He may also have even been the first person to posit the existence of black holes--celestial bodies whose gravity is so great that even light cannot escape. However, here, we are most interested in the work he did on inverse probability theory.

Independent of Bayes, Laplace had already began to work on a probabilistic way to go from effect back to cause, and in 1774 published \textit{M{\'e}moire sur la probabilit{\'e} des causes par les {\'e}v{\`e}nemens} in which he stated the principle:

If an event can be produced by a number $n$ of different causes, then the probabilities of these causes given the event are to each other as the probabilities of the event given the causes, and the probability of the existence of each of these is equal to the probability of the event given the cause, divided by the sum of all the probabilities of the event given each of these causes.

This statement of inverse probability is only valid when the causes are all equally likely. It was not until later on Laplace generalised this result to handle causes with different prior weights.

In 1781, Price visited Paris and told the Secretary of the French Royal Academy of Sciences, the Marquis of Condorcet, about Bayes' discovery. This information eventually reached Laplace and gave him confidence to pursue his ideas in inverse probability. The trouble with his theory for going from an effect back to a cause was that it required an enormous number of calculations to be done to arrive at an answer. Laplace was not afraid of a challenge, however, and invented a number of incredibly useful techniques (for example , generating functions and transforms) to find an approximate answer. Laplace still needed an example application of his method that was easy enough for him to calculate, yet interesting enough to garner attention. His chose data sample was composed of babies. Specifically, his sample comprised the number of males and females born in Paris from 1745 to 1770. This data was easy to work with because the outcome was binary--the child was recorded as being born a boy or girl--and was large enough to be able to draw conclusions from it. In the sample, a total of 241,945 girls and 251,527 boys were born. Laplace used this sample and his theory of inverse probability to estimate that there was a probability of approximately $10^{-42}$ that the sex ratio favoured girls rather than boys. On the basis of this tiny probability, he concluded that he was as ``certain as any other moral truth'' that boys were born more frequently than girls. This was the first practical application of Bayes inference as we know it now. Laplace went from an effect--the data in the birth records--to determine a probable cause--the ratio of male to female births.

Later in his life, Laplace also wrote down the first modern version of Bayes' mathematical rle that is used today, where causes could be given different prior probabilities. He published it in his \textit{Th{\'e}orie analytique des probabilit{\'e}s} in 1820 (although he probably derived the rule around 1810--1814):
\[ P=\frac{Hp}{S. Hp} \]

On the left-hand side, $P$ denotes the posterior probability of a given cause given an observed event. In the numerator on the right-hand side, $H$ is the probability of an event occurring given that cause, $p$, is the a priori probability of that cause. In the denominator, $S.$ denotes summation (the modern equivalent of this is $\Sigma$) over all possible causes, and $H$ and $p$ now represent the corresponding quantities to those in the numerator, but for each possible cause. Laplace actually presented two versions of the rule--one for discrete random variables and another for continuous variables. The typesetting he used for the continuous case, however, did not allow him to write limits on integrals, meaning that the numerator and denominator look the same.

\end{document}